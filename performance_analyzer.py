"""
æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å·¥å…·
ç”¨äºåˆ†æå¤ç›˜ç³»ç»Ÿçš„æ€§èƒ½ç“¶é¢ˆ
"""

import time
import psutil
import os
from pathlib import Path
from functools import wraps
import pandas as pd


def timing_decorator(func):
    """è®¡æ—¶è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"â±ï¸  {func.__name__} è€—æ—¶: {end - start:.2f}ç§’")
        return result
    return wrapper


class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.process = psutil.Process(os.getpid())
        
    def get_memory_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        mem_info = self.process.memory_info()
        mem_mb = mem_info.rss / 1024 / 1024
        return mem_mb
    
    def get_cpu_usage(self):
        """è·å–CPUä½¿ç”¨ç‡"""
        return self.process.cpu_percent(interval=1)
    
    def analyze_data_size(self, data_dir):
        """åˆ†ææ•°æ®å¤§å°"""
        data_path = Path(data_dir)
        
        print("=" * 60)
        print("ğŸ“Š æ•°æ®å¤§å°åˆ†æ")
        print("=" * 60)
        
        # ç»Ÿè®¡æ–‡ä»¶æ•°é‡å’Œå¤§å°
        parquet_files = list(data_path.glob("*.parquet"))
        
        if not parquet_files:
            print("æœªæ‰¾åˆ°parquetæ–‡ä»¶")
            return
        
        total_size = sum(f.stat().st_size for f in parquet_files)
        avg_size = total_size / len(parquet_files)
        
        print(f"æ–‡ä»¶æ•°é‡: {len(parquet_files)}")
        print(f"æ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB")
        print(f"å¹³å‡å¤§å°: {avg_size / 1024:.2f} KB")
        
        # é‡‡æ ·åˆ†æ
        sample_size = min(10, len(parquet_files))
        sample_files = parquet_files[:sample_size]
        
        print(f"\nğŸ“‹ é‡‡æ ·åˆ†æ (å‰{sample_size}ä¸ªæ–‡ä»¶):")
        print("-" * 60)
        
        total_rows = 0
        for file_path in sample_files:
            df = pd.read_parquet(file_path)
            rows = len(df)
            size_kb = file_path.stat().st_size / 1024
            total_rows += rows
            
            print(f"{file_path.stem}: {rows:6,} è¡Œ | {size_kb:8.2f} KB")
        
        avg_rows = total_rows / sample_size
        estimated_total_rows = avg_rows * len(parquet_files)
        
        print("-" * 60)
        print(f"å¹³å‡æ¯æ–‡ä»¶: {avg_rows:,.0f} è¡Œ")
        print(f"ä¼°è®¡æ€»è¡Œæ•°: {estimated_total_rows:,.0f} è¡Œ")
        
        return {
            'file_count': len(parquet_files),
            'total_size_mb': total_size / 1024 / 1024,
            'avg_rows': avg_rows,
            'estimated_total_rows': estimated_total_rows,
        }
    
    @timing_decorator
    def benchmark_loading(self, data_dir, sample_size=100):
        """åŸºå‡†æµ‹è¯•: æ•°æ®åŠ è½½é€Ÿåº¦"""
        print("\n" + "=" * 60)
        print("âš¡ åŠ è½½é€Ÿåº¦åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        data_path = Path(data_dir)
        parquet_files = list(data_path.glob("*.parquet"))[:sample_size]
        
        start_mem = self.get_memory_usage()
        
        # æµ‹è¯•åŠ è½½
        dfs = []
        for file_path in parquet_files:
            df = pd.read_parquet(file_path)
            dfs.append(df)
        
        end_mem = self.get_memory_usage()
        mem_increase = end_mem - start_mem
        
        print(f"åŠ è½½æ–‡ä»¶æ•°: {sample_size}")
        print(f"å†…å­˜å¢åŠ : {mem_increase:.2f} MB")
        print(f"å¹³å‡æ¯æ–‡ä»¶: {mem_increase / sample_size:.2f} MB")
        
        # ä¼°ç®—å…¨é‡åŠ è½½å†…å­˜éœ€æ±‚
        total_files = len(list(data_path.glob("*.parquet")))
        estimated_mem = mem_increase / sample_size * total_files
        
        print(f"\nğŸ’¡ å…¨é‡åŠ è½½ä¼°ç®—:")
        print(f"é¢„è®¡æ€»å†…å­˜: {estimated_mem:.2f} MB ({estimated_mem / 1024:.2f} GB)")
        
        return {
            'sample_size': sample_size,
            'mem_increase_mb': mem_increase,
            'estimated_total_mem_gb': estimated_mem / 1024,
        }
    
    @timing_decorator
    def benchmark_ranking_calculation(self, data_dir):
        """åŸºå‡†æµ‹è¯•: æ’è¡Œæ¦œè®¡ç®—é€Ÿåº¦"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ æ’è¡Œæ¦œè®¡ç®—åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        from replay_engine import ReplayEngine
        from datetime import datetime
        
        engine = ReplayEngine(data_dir)
        
        # åŠ è½½éƒ¨åˆ†æ•°æ®
        data_path = Path(data_dir)
        parquet_files = list(data_path.glob("*.parquet"))[:500]
        
        print(f"åŠ è½½ {len(parquet_files)} åªè‚¡ç¥¨æ•°æ®...")
        for file_path in parquet_files:
            engine.lazy_load_stock(file_path.stem)
        
        # æµ‹è¯•å¿«ç…§ç”Ÿæˆ
        test_time = datetime(2025, 12, 16, 10, 30, 0)
        
        start = time.time()
        snapshot = engine.get_snapshot_at_time(test_time)
        snapshot_time = time.time() - start
        
        print(f"å¿«ç…§ç”Ÿæˆ: {snapshot_time:.3f}ç§’")
        
        # æµ‹è¯•æ’è¡Œè®¡ç®—
        start = time.time()
        stock_rankings = engine.calculate_stock_rankings(snapshot, top_n=30)
        ranking_time = time.time() - start
        
        print(f"ä¸ªè‚¡æ’è¡Œ: {ranking_time:.3f}ç§’")
        
        # æµ‹è¯•æ‹‰å‡æ£€æµ‹
        start = time.time()
        rapid_rise = engine.detect_rapid_rise(time_window_minutes=5, pct_threshold=3.0)
        rapid_time = time.time() - start
        
        print(f"æ‹‰å‡æ£€æµ‹: {rapid_time:.3f}ç§’")
        
        total_time = snapshot_time + ranking_time + rapid_time
        
        print(f"\næ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"é¢„ä¼°FPS: {1 / total_time:.1f} æ¬¡/ç§’")
        
        return {
            'snapshot_time': snapshot_time,
            'ranking_time': ranking_time,
            'rapid_time': rapid_time,
            'total_time': total_time,
        }
    
    def generate_optimization_report(self, data_dir):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        print("=" * 60)
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        total_mem = psutil.virtual_memory().total / 1024 / 1024 / 1024
        available_mem = psutil.virtual_memory().available / 1024 / 1024 / 1024
        cpu_count = psutil.cpu_count()
        
        print(f"\nğŸ–¥ï¸  ç³»ç»Ÿé…ç½®:")
        print(f"CPUæ ¸å¿ƒæ•°: {cpu_count}")
        print(f"æ€»å†…å­˜: {total_mem:.1f} GB")
        print(f"å¯ç”¨å†…å­˜: {available_mem:.1f} GB")
        
        # åˆ†ææ•°æ®
        data_stats = self.analyze_data_size(data_dir)
        
        if data_stats:
            estimated_mem_gb = data_stats.get('estimated_total_mem_gb', 0)
            
            print(f"\nğŸ“Š æ•°æ®åˆ†æ:")
            print(f"è‚¡ç¥¨æ•°é‡: {data_stats['file_count']}")
            print(f"æ•°æ®å¤§å°: {data_stats['total_size_mb']:.2f} MB")
            print(f"ä¼°è®¡æ€»è¡Œæ•°: {data_stats['estimated_total_rows']:,.0f}")
            
            print(f"\nğŸ’¾ å†…å­˜éœ€æ±‚:")
            print(f"å…¨é‡åŠ è½½éœ€è¦: {estimated_mem_gb:.2f} GB")
            
            # ç”Ÿæˆå»ºè®®
            print(f"\nâœ… ä¼˜åŒ–å»ºè®®:")
            
            if estimated_mem_gb > available_mem * 0.8:
                print("1. âš ï¸  å†…å­˜å¯èƒ½ä¸è¶³,å»ºè®®ä½¿ç”¨æŒ‰éœ€åŠ è½½ç­–ç•¥")
                print("   - åœ¨ replay_engine.py ä¸­ä¸è°ƒç”¨ load_all_data()")
                print("   - æˆ–å‡å°‘åŒæ—¶åˆ†æçš„è‚¡ç¥¨æ•°é‡")
            else:
                print("1. âœ… å†…å­˜å……è¶³,å¯ä»¥ä½¿ç”¨å…¨é‡åŠ è½½ç­–ç•¥")
                print("   - åœ¨ replay_engine.py ä¸­è°ƒç”¨ load_all_data()")
            
            if cpu_count >= 8:
                print(f"2. âœ… CPUæ ¸å¿ƒå……è¶³({cpu_count}æ ¸),å¯å¢åŠ ä¸‹è½½çº¿ç¨‹æ•°")
                print(f"   - max_workers å¯è®¾ç½®ä¸º {cpu_count * 2}-{cpu_count * 3}")
            else:
                print(f"2. âš ï¸  CPUæ ¸å¿ƒè¾ƒå°‘({cpu_count}æ ¸),å»ºè®®é€‚åº¦å¹¶å‘")
                print(f"   - max_workers å»ºè®®è®¾ç½®ä¸º {cpu_count}-{cpu_count * 2}")
            
            if data_stats['avg_rows'] > 10000:
                print("3. ğŸ’¡ åˆ†æ—¶æ•°æ®è¾ƒå¤š,å»ºè®®:")
                print("   - ä½¿ç”¨ parquet å‹ç¼©æ ¼å¼(å·²é»˜è®¤)")
                print("   - è€ƒè™‘æ•°æ®é™é‡‡æ ·(å¦‚æ¯5ç§’ä¸€ä¸ªç‚¹)")
            
            print("\n4. ğŸš€ æ€§èƒ½ä¼˜åŒ–æŠ€å·§:")
            print("   - ä½¿ç”¨SSDå­˜å‚¨æ•°æ®")
            print("   - å…³é—­ä¸å¿…è¦çš„åå°ç¨‹åº")
            print("   - å‡å°‘æ’è¡Œæ¦œæ˜¾ç¤ºæ•°é‡")
            print("   - å¢åŠ å›æ”¾é€Ÿåº¦é—´éš”(å¦‚5-10ç§’)")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ” Aè‚¡å¤ç›˜ç³»ç»Ÿ - æ€§èƒ½åˆ†æå·¥å…·")
    print("=" * 60)
    
    # é€‰æ‹©æ•°æ®ç›®å½•
    data_dirs = list(Path("data").glob("tick_*"))
    
    if not data_dirs:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®ç›®å½•")
        return
    
    print("\nå¯ç”¨çš„æ•°æ®ç›®å½•:")
    for idx, dir_path in enumerate(data_dirs, 1):
        print(f"{idx}. {dir_path.name}")
    
    choice = input(f"\nè¯·é€‰æ‹© (1-{len(data_dirs)}): ").strip()
    
    try:
        selected_dir = data_dirs[int(choice) - 1]
    except (ValueError, IndexError):
        print("æ— æ•ˆé€‰æ‹©")
        return
    
    # æ‰§è¡Œåˆ†æ
    analyzer = PerformanceAnalyzer()
    
    print(f"\næ­£åœ¨åˆ†æ: {selected_dir}")
    
    # 1. æ•°æ®å¤§å°åˆ†æ
    data_stats = analyzer.analyze_data_size(selected_dir)
    
    # 2. åŠ è½½é€Ÿåº¦æµ‹è¯•
    if input("\næ˜¯å¦è¿›è¡ŒåŠ è½½é€Ÿåº¦æµ‹è¯•? (y/n): ").strip().lower() == 'y':
        sample_size = int(input("æµ‹è¯•æ ·æœ¬å¤§å° (å»ºè®®100-500): ").strip() or "100")
        analyzer.benchmark_loading(selected_dir, sample_size=sample_size)
    
    # 3. è®¡ç®—é€Ÿåº¦æµ‹è¯•
    if input("\næ˜¯å¦è¿›è¡Œè®¡ç®—é€Ÿåº¦æµ‹è¯•? (y/n): ").strip().lower() == 'y':
        analyzer.benchmark_ranking_calculation(selected_dir)
    
    # 4. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    analyzer.generate_optimization_report(selected_dir)
    
    print("\n" + "=" * 60)
    print("âœ… åˆ†æå®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
